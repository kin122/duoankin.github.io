精确搜索和全文搜索是使用 ES 进行搜索时基本且常见的一个话题。  
精确搜索往往有着**查询逻辑简单、查询效率较高、方便理解**的优势，但也对内容的搜索方式限制较大，**一般只能使用一个词对某一个字段进行搜索**，且搜索的结果是否命中与文档索引时分词器的设置或者选择关联较大。  
而全文搜索则与之相反，内容搜索的方式**灵活且多样**，可以**不同的内容在 不同的字段同时进行搜索**，甚至能根据复杂的场景制定不同的搜索方法以满足不同的查询需求，**相比精确搜索更容易搜索出想要的结果**。同时也存在，查询逻辑复杂、运用起来不易理解、搜索效率相对较低的缺点。  
下面我们来比对一下精确搜索最常见的使用方法 term 与全文搜索最常见的使用方法 match 之间的差别。
### term 与 match  
```
#先创建一个测试索引并放入两条数据
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_text": { "type": "text" }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "full_text":   "quick brown foxes!"
}

PUT my-index-000001/_doc/2
{
  "full_text":   "Quick Foxes Brown !"
}
```
我们先使用 term 查询搜索一下关键词 foxes 。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "foxes"
    }
  }
}
```
返回结果为
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
可以发现两条记录都命中了。我们再试试使用 foxes 后添加一个感叹号形成"foxes!"搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "foxes!"
    }
  }
}
```
返回结果为空
```
{
  ···
    "hits" : [ ]
  }
}

```
我们再使用"brown foxes"这两个关键词试验一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "brown foxes"
    }
  }
}
```
会发现返回结果也是为空。  
```
{
  ···
    "hits" : [ ]
  }
}

```
上述三种查询关键词，只有 foxes 顺利能搜索返回相关文档，其它稍有变化的关键词或者组合的关键词都没有结果返回。  
下面我们看看 match。先使用 foxes 搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes"
    }
  }
}
```
正常返回两条结果。
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
再使用"foxes!"搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes!"
    }
  }
}
```
也正常返回两条结果（节省篇幅此处不展示）。  
再使用两个关键词组合的"brown foxes"进行搜索。  
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "brown foxes"
    }
  }
}
```
这里还是返回两条结果，而不是按照两个关键词顺序更匹配的文档 1 。  
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
最后我们将两个关键词顺序颠倒一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes brown "
    }
  }
}
```
依旧返回两条结果，并且连顺序都一样。 
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
从上面的例子可以窥探出精确搜索和全文搜索的一些特点：精确搜索一旦对搜索关键词把控不好就很有可能得不到搜索结果；而全文搜索虽然容易查出结果，但也往往会缺乏精度，返回过多的结果。  
那么，大家是否会有下面的问题：
1. 精确搜索究竟怎么查询才能匹配出更多的结果？
2. 全文搜索中有什么办法可以把搜索结果范围缩小？或者搜索出来的结果可以按照更高的匹配度排序？
3. 这两者查询背后究竟有什么样的原理？

我们进入下一环节。
### 原理与实践
在解析精确搜索和全文搜索之前，我们先来理解两个在 ES 搜索查询中需要知道的两个概念 term 和 token 。  
**Term** 代表文本中的一个单词。这是搜索单元，即搜索的最小元素(可以称之为词项)。
**Token** 文档进行索引的后最小元素，即语汇单元。token 一般由项（term）、位置（position）、偏移量（offset）、类型（type）、标志位（flags）和有效负载这些要素组成。    
token的产生其实也是倒排索引产生的过程，是通过一定的解析规则将一段文档解析成一堆不可再拆分的词干的过程。  
我们可以通过 \_analyze API 体验到文档解析成 token 的过程。  
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "Quick Brown Foxes!"
  ]
}

#返回结果
{
  "tokens" : [
    {
      "token" : "quick",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "brown",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "foxes",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}

```
通过 API 可以看到，文档内容"Quick Brown Foxes!"经过 standard 分词器解析后形成 "quick","brown","foxes" 三个 token 。  
也可以看到每个 token 的 offset、type 和 position 的属性。  

```
https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/index/Term.html  
在 ES 中，Term 查询，对输⼊不做分词。会将输⼊作为⼀个整体，在倒排索引中查找准确的词项，并且使⽤相关度算分公式为每个包含该词项的⽂档进⾏相关度算分。  
term 查询在 text 文档中的实现精确查询，需要 term 与文档完全对应。  
https://www.zhihu.com/question/32133683  
https://lucene.apache.org/core/8_9_0/core/index.html
Plain text passed to Lucene for indexing goes through a process generally called tokenization. Tokenization is the process of breaking input text into small indexing elements – tokens. The way input text is broken into tokens heavily influences how people will then be able to search for that text. For instance, sentences beginnings and endings can be identified to provide for more accurate phrase and proximity searches (though sentence identification is not provided by Lucene).   
**tokenization**传递给Lucene用于索引的纯文本经过一个通常称为标记化的过程。标记化是将输入文本分解为小索引元素——标记的过程。输入文本被分割成符号的方式很大程度上影响了人们搜索该文本的方式。例如，可以识别句子的开头和结尾，以提供更准确的短语和邻近搜索(尽管Lucene不提供句子识别)。  
https://lucene.apache.org/core/8_9_0/core/index.html  
```
那我们先看看搜索与索引时的处理过程：
![索引与搜索](https://github.com/kin122/duoankin.github.io/blob/main/elasticsearch/%E6%90%9C%E7%B4%A2%E4%B8%8E%E7%B4%A2%E5%BC%95.png)   
注意：搜索和索引的过程中，分词器可以选择不一样的，虽然并不推荐这么做。  

现在我们可以看一下 ES 官方对全文搜索（full text queries）和精确搜索(term-level queries)的定义：
* 全文搜索：可以对多个字段同时搜索可被分词器解析的文本内容。 
* 精确搜索：将搜索内容作为精确值去查找文档，精确搜索不会对搜索内容进行分词器解析，并且只能对单个字段进行词项匹配。

两者之间的比对如下图：
![精确搜索与全文搜索](https://github.com/kin122/duoankin.github.io/blob/main/elasticsearch/%E7%B2%BE%E7%A1%AE%E6%90%9C%E7%B4%A2%E4%B8%8E%E5%85%A8%E6%96%87%E6%90%9C%E7%B4%A2.png)   

：
1. 精确搜索只能查询一个 field ，而全文搜索可以多个字段同时搜索
2. term 是将搜索内容直接作为一个最小单元与 token 的信息进行比对；全文搜索则是进行语法和分词器解析后，将解析到的 term 进行与文档索引后得到的 token 进行词项比对。
3. term 查询方式更加简单，确定了字段和比对的词项。而全文搜索中这两者可能会随着语法和分词器的变化而变化。
4. 从 lucene 源码实现的角度来说，term 查询的实现类是 TermQuery ，全文搜索使用的是 QueryParser 类，而 QueryParser 最终默认使用 TermQuery 进行多个 Term 的关系查询。

#### match不匹配情况  
使用analyze查看token解析的情况

### 精确搜索的相关方法
#### term
参数  
* boost
可以通过 Constant Score 将查询转换成⼀个 Filtering，避免算分，并利⽤缓存，提⾼性能  
#### terms
terms lookup  
#### terms_set
参数  
* minimum_should_match_field
* minimum_should_match_script
#### wildcard

#### range
* range 查询 text 或者 keyword
* range 查询 date 
  * 时区的用法
* range 查询 math 和 rounding
#### fuzzy
参数：
* fuzziness
* max_expansions
* prefix_length
* transpositions
#### prefix

#### regexp
max_determinized_states
#### ids

#### exists

### 全文搜索的相关方法
#### match
参数：
* analyzer
* auto_generate_synonyms_phrase_query
* fuzziness
* max_expansions
* prefix_length
* fuzzy_transpositions
* fuzzy_rewrite
* lenient
* operator
* minimum_should_match
* zero_terms_query
#### match_phrase
* slop 
* analyzer 
* zero_terms_query
#### match_phrase_prefix

#### multi_match
相关查询方式：
* best_fields
* most_fields
* cross_fields
* phrase
* phrase_prefix
* bool_prefix
#### match_bool_prefix

#### query_string
* query string syntax
* field names
* wildcards
* regular expressions
* fuzziness
* proximity search
* ranges
* boosting
* boolean operators
* grouping
* reserved characters
* search multiple fields
* synonyms 
* minimum_should_match
#### simple_query_string
* simple query string syntax
* limit operators
* wildcards and per-field boosts in the fields parameter
* multi-position tokens
#### intervals
* match
* prefix
* wildcard
* fuzzy
* all_of
* any_of
#### combined fields
与 multi_match 的比较
### lucene中的精确搜索和全文匹配
* token的要素
* tokenization
* parser
#### lucene的查询基本类
#### lucene的queryparser

