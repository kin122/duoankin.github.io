### term查询
Term 是表达语意的最⼩单位。搜索和利⽤统计语⾔模型进⾏⾃然语⾔处理都需要处理 Term。  
https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/index/Term.html  
在 ES 中，Term 查询，对输⼊不做分词。会将输⼊作为⼀个整体，在倒排索引中查找准确的词项，并且使⽤相关度算分公式为每个包含该词项的⽂档进⾏相关度算分。  
term 查询在 text 文档中的实现l1精确查询，需要 term 与文档完全对应。  
实现案例  
```
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_text": { "type": "text" }
    }
  }
}
PUT my-index-000001/_doc/1
{
  "full_text":   "Quick Brown Foxes!"
}
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "Quick Brown Foxes!"
    }
  }
}
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "Quick Brown Foxes!"
    }
  }
}
```
### match查询
索引和搜索时都会进⾏分词，查询字符串先传递到⼀个合适的分词器，然后⽣成⼀个供查询的词项列表  
查询时候，先会对输⼊的查询进⾏分词，然后每个词项逐个进⾏底层的查询，最终将结果进⾏合并。并为每个⽂档⽣成⼀个算分。  
查询主要参数：
* Operator  
* minimum_should_match  
### 原理与实践
#### term token fields之间的联系
https://www.zhihu.com/question/32133683  
https://lucene.apache.org/core/8_9_0/core/index.html
Plain text passed to Lucene for indexing goes through a process generally called tokenization. Tokenization is the process of breaking input text into small indexing elements – tokens. The way input text is broken into tokens heavily influences how people will then be able to search for that text. For instance, sentences beginnings and endings can be identified to provide for more accurate phrase and proximity searches (though sentence identification is not provided by Lucene).   
传递给Lucene用于索引的纯文本经过一个通常称为标记化的过程。标记化是将输入文本分解为小索引元素——标记的过程。输入文本被分割成符号的方式很大程度上影响了人们搜索该文本的方式。例如，可以识别句子的开头和结尾，以提供更准确的短语和邻近搜索(尽管Lucene不提供句子识别)。  
https://lucene.apache.org/core/8_9_0/core/index.html  
A field that is indexed and tokenized, without term vectors. For example this would be used on a 'body' field, that contains the bulk of a document's text.

#### match不匹配情况  


### 精确搜索的相关方法
#### term
参数  
* boost
可以通过 Constant Score 将查询转换成⼀个 Filtering，避免算分，并利⽤缓存，提⾼性能  
#### terms
terms lookup  
#### terms_set
参数  
* minimum_should_match_field
* minimum_should_match_script
#### wildcard

#### range
* range 查询 text 或者 keyword
* range 查询 date 
  * 时区的用法
* range 查询 math 和 rounding
#### fuzzy
参数：
* fuzziness
* max_expansions
* prefix_length
* transpositions
#### prefix

#### regexp
max_determinized_states
#### ids

#### exists

### 全文搜索的相关方法
#### match
参数：
* analyzer
* auto_generate_synonyms_phrase_query
* fuzziness
* max_expansions
* prefix_length
* fuzzy_transpositions
* fuzzy_rewrite
* lenient
* operator
* minimum_should_match
* zero_terms_query
#### match_phrase
* slop 
* analyzer 
* zero_terms_query
#### match_phrase_prefix

#### multi_match
相关查询方式：
* best_fields
* most_fields
* cross_fields
* phrase
* phrase_prefix
* bool_prefix
#### match_bool_prefix

#### query_string
* query string syntax
* field names
* wildcards
* regular expressions
* fuzziness
* proximity search
* ranges
* boosting
* boolean operators
* grouping
* reserved characters
* search multiple fields
* synonyms 
* minimum_should_match
#### simple_query_string
* simple query string syntax
* limit operators
* wildcards and per-field boosts in the fields parameter
* multi-position tokens
#### intervals
* match
* prefix
* wildcard
* fuzzy
* all_of
* any_of
#### combined fields
与 multi_match 的比较
### lucene中的精确搜索和全文匹配
#### lucene的查询基本类
#### lucene的queryparser

