精确搜索和全文搜索是使用 ES 进行搜索时基本且常见的一个话题。  
精确搜索往往有着**查询逻辑简单、查询效率较高、方便理解**的优势，但也对内容的搜索方式限制较大，**一般只能使用一个词对某一个字段进行搜索**，且搜索的结果是否命中与文档索引时分词器的设置或者选择关联较大。  
而全文搜索则与之相反，内容搜索的方式**灵活且多样**，可以**不同的内容在 不同的字段同时进行搜索**，甚至能根据复杂的场景制定不同的搜索方法以满足不同的查询需求，**相比精确搜索更容易搜索出想要的结果**。同时也存在，查询逻辑复杂、运用起来不易理解、搜索效率相对较低的缺点。  
下面我们来比对一下精确搜索最常见的使用方法 term 与全文搜索最常见的使用方法 match 之间的差别。
### 1 term 与 match  
```
#先创建一个测试索引并放入两条数据
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_text": { "type": "text" }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "full_text":   "quick brown foxes!"
}

PUT my-index-000001/_doc/2
{
  "full_text":   "Quick Foxes Brown !"
}
```
我们先使用 term 查询搜索一下关键词 foxes 。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "foxes"
    }
  }
}
```
返回结果为
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
可以发现两条记录都命中了。我们再试试使用 foxes 后添加一个感叹号形成"foxes!"搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "foxes!"
    }
  }
}
```
返回结果为空
```
{
  ···
    "hits" : [ ]
  }
}

```
我们再使用"brown foxes"这两个关键词试验一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "brown foxes"
    }
  }
}
```
会发现返回结果也是为空。  
```
{
  ···
    "hits" : [ ]
  }
}

```
上述三种查询关键词，只有 foxes 顺利能搜索返回相关文档，其它稍有变化的关键词或者组合的关键词都没有结果返回。  
下面我们看看 match。先使用 foxes 搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes"
    }
  }
}
```
正常返回两条结果。
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
再使用"foxes!"搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes!"
    }
  }
}
```
也正常返回两条结果（节省篇幅此处不展示）。  
再使用两个关键词组合的"brown foxes"进行搜索。  
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "brown foxes"
    }
  }
}
```
这里还是返回两条结果，而不是按照两个关键词顺序更匹配的文档 1 。  
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
最后我们将两个关键词顺序颠倒一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes brown "
    }
  }
}
```
依旧返回两条结果，并且连顺序都一样。 
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
从上面的例子可以窥探出精确搜索和全文搜索的一些特点：精确搜索一旦对搜索关键词把控不好就很有可能得不到搜索结果；而全文搜索虽然容易查出结果，但也往往会缺乏精度，返回过多的结果。  
那么，大家是否会有下面的问题：
1. 精确搜索究竟怎么查询才能匹配出更多的结果？
2. 全文搜索中有什么办法可以把搜索结果范围缩小？或者搜索出来的结果可以按照更高的匹配度排序？
3. 这两者查询背后究竟有什么样的原理？

我们进入下一环节。
### 2.1 原理
在解析精确搜索和全文搜索之前，我们先来理解两个在 ES 搜索查询中需要知道的两个概念 term 和 token 。  
**Term** 代表文本中的一个单词。这是搜索单元，即搜索的最小元素(可以称之为词项)。  
**Token** 文档进行索引的后最小元素，即语汇单元。Token 一般由项（term）、位置（position）、偏移量（offset）、类型（type）、标志位（flags）和有效负载这些要素组成。    
Token 的产生其实也是倒排索引产生的过程，是通过一定的解析规则将一段文档解析成一堆不可再拆分的词干的过程。  
而 ES 搜索的最终过程就是 term 与 token 进行比对的过程（我们在这里称它为词项比对）。  
我们可以通过 \_analyze API 体验到文档解析成 token 的过程。  
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "Quick Foxes Brown !"
  ]
}

#返回结果
{
  "tokens" : [
    {
      "token" : "quick",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "foxes",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "brown",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}

```
通过 API 可以看到，文档内容"Quick Brown Foxes!"经过 standard 分词器解析后形成 "quick","brown","foxes" 三个 token 。  
也可以看到每个 token 的 offset、type 和 position 的属性。  

```
https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/index/Term.html  
在 ES 中，Term 查询，对输⼊不做分词。会将输⼊作为⼀个整体，在倒排索引中查找准确的词项，并且使⽤相关度算分公式为每个包含该词项的⽂档进⾏相关度算分。  
term 查询在 text 文档中的实现精确查询，需要 term 与文档完全对应。  
https://www.zhihu.com/question/32133683  
https://lucene.apache.org/core/8_9_0/core/index.html
Plain text passed to Lucene for indexing goes through a process generally called tokenization. Tokenization is the process of breaking input text into small indexing elements – tokens. The way input text is broken into tokens heavily influences how people will then be able to search for that text. For instance, sentences beginnings and endings can be identified to provide for more accurate phrase and proximity searches (though sentence identification is not provided by Lucene).   
**tokenization**传递给Lucene用于索引的纯文本经过一个通常称为标记化的过程。标记化是将输入文本分解为小索引元素——标记的过程。输入文本被分割成符号的方式很大程度上影响了人们搜索该文本的方式。例如，可以识别句子的开头和结尾，以提供更准确的短语和邻近搜索(尽管Lucene不提供句子识别)。  
https://lucene.apache.org/core/8_9_0/core/index.html  
```
那我们先看看搜索与索引时的处理过程：
![索引与搜索](https://github.com/kin122/duoankin.github.io/blob/main/elasticsearch/%E6%90%9C%E7%B4%A2%E4%B8%8E%E7%B4%A2%E5%BC%95.png)   
注意：搜索和索引的过程中，分词器可以选择不一样的，虽然并不推荐这么做。  

现在我们可以看一下 ES 官方对全文搜索（full text queries）和精确搜索(term-level queries)的定义：
* 全文搜索：可以对多个字段同时搜索可被分词器解析的文本内容。 
* 精确搜索：将搜索内容作为精确值去查找文档，精确搜索不会对搜索内容进行分词器解析，并且只能对单个字段进行词项匹配。

两者之间的比对如下图：
![精确搜索与全文搜索](https://github.com/kin122/duoankin.github.io/blob/main/elasticsearch/%E7%B2%BE%E7%A1%AE%E6%90%9C%E7%B4%A2%E4%B8%8E%E5%85%A8%E6%96%87%E6%90%9C%E7%B4%A2.png)   

可以得出以下几点：
1. 精确搜索只能查询一个 field ，而全文搜索可以多个字段同时搜索
2. term 是将搜索内容直接作为一个最小单元与 token 的信息进行比对；全文搜索则是进行语法和分词器解析后，将解析到的 term 进行与文档索引后得到的 token 进行词项比对。
3. term 查询方式更加简单，确定了字段和比对的词项。而全文搜索中这两者可能会随着语法和分词器的变化而变化。
4. 从 lucene 源码实现的角度来说，term 查询的实现类是 TermQuery ，全文搜索使用的是 QueryParser 类，而 QueryParser 最终默认使用 TermQuery 进行多个 Term 的关系查询。

#### 2.2 实践
现在让我们重新看一下之前的 term 和 match 查询。  
首先看一下测试索引中的两个文档经过分词后的词项情况，由于之前文档 \_id 为2已经被解析过了，我们看一下文档 \_id 为1的情况：
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "quick brown foxes!"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "quick",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "brown",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "foxes",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}

```
可见三个 token 的词项内容是一样的，其它参数如 offset、position 等有了相应的变化。 
我们先看精确搜索的条件：  
第一次，term 查询了"foxes"，词项就为“foxes”，该词项存在于两个文档分词解析后的 token 中，所以可以两条都可返回。  
第二次，term 查询了"foxes!",词项为 “foxes！”，该词项与“foxes”并不完全一致，多了一个感叹号，所以命中为空。  
第三次，term 查询了"brown foxes"，词项为“brown foxes”，该词项并不与两个文档的任何token匹配，所以命中也为空。  
通过上面的分析，我们可以看出，**term 查询如果需要能正确匹配出想要的文档，那么查询的词项必须是与文档索引时分词出的 token 内容相一致**。  

现在我们看看 match 的全文搜索：  
第一次为 match 查询“foxes”，先使用 analyze API 查看下分词词根，注意之前索引文档的时候未指定索引分词器和搜索分词器，那么 ES 会采用默认的 standard 分词器。  
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "foxes"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "foxes",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    }
  ]
}
```
可以看出，分词器对搜索的内容没有做什么改变，因此两条文档全部命中也是很容易理解。  
第二次，match 查询“foxes!”，也使用 analyze API 查看下。  
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "foxes!"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "foxes",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    }
  ]
}
```
这次分词器把“foxes!”解析成“foxes”，因此两条文档也全部命中。  
我们再看看第三第四次 match 的内容：
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "brown foxes"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "brown",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "foxes",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}

POST _analyze
{
  "analyzer": "standard",
  "text": [
    "foxes brown "
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "foxes",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "brown",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```
可见两个短语在被分词器解析后，两个词项和文档的 token 都能匹配，因此两条都能返回。  
由于 match 查询并没有词项的位置关系计算，因此两个搜索内容的效果是一样的，如果想要对全文搜索内容做更加位置关系的计算让其看上去更匹配，则可以使用 match_phrase，后面讲解全文搜索具体方法时会有相关详细解释。  

### 3 精确搜索的相关方法
精确搜索主要有 term/terms/terms_set/wildcard/range/fuzzy/prefix/regexp/ids/exists 十个主要查询方法。  
#### 3.1 term
term 查询是精确搜索最常见最基础的一种方式。  
日常用法如下：
```
GET /_search
{
  "query": {
    "term": {
      "user.id": { # 字段名
        "value": "kimchy", # 被搜索的词项
        "boost": 1.0
      }
    }
  }
}
```
主要参数：  
* boost：用于减少或增加查询的相关分数的浮点数值。默认为1.0。可以使用boost参数来调整包含两个或多个查询的搜索的相关性分数。Boost值相对于默认值1.0。0到1.0之间的升压值会降低相关性得分。大于1.0的值会增加相关分数.


使用建议：可以通过 Constant Score 将查询转换成⼀个 Filter，避免算分，并利⽤缓存，提⾼性能。  
```
# 原查询
GET my-index-000001/_search
{"explain": true,  # 打开 explain 查看算分逻辑
  "query": {
    "term": {
      "full_text": "foxes"
    }
  }
}
# 相关返回
{
  ......
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.18232156,
    "hits" : [
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        },
        "_explanation" : {
          "value" : 0.18232156,
          "description" : "weight(full_text:foxes in 0) [PerFieldSimilarity], result of:",
          "details" : [
            {
              "value" : 0.18232156,
              "description" : "score(freq=1.0), computed as boost * idf * tf from:",
              "details" : [
                {
                  "value" : 2.2,
                  "description" : "boost",
                  "details" : [ ]
                },
                {
                  "value" : 0.18232156,
                  "description" : "idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:",
                  "details" : [
                    {
                      "value" : 2,
                      "description" : "n, number of documents containing term",
                      "details" : [ ]
                    },
                    {
                      "value" : 2,
                      "description" : "N, total number of documents with field",
                      "details" : [ ]
                    }
                  ]
                },
                {
                  "value" : 0.45454544,
                  "description" : "tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:",
                  "details" : [
                    {
                      "value" : 1.0,
                      "description" : "freq, occurrences of term within document",
                      "details" : [ ]
                    },
                    {
                      "value" : 1.2,
                      "description" : "k1, term saturation parameter",
                      "details" : [ ]
                    },
                    {
                      "value" : 0.75,
                      "description" : "b, length normalization parameter",
                      "details" : [ ]
                    },
                    {
                      "value" : 3.0,
                      "description" : "dl, length of field",
                      "details" : [ ]
                    },
                    {
                      "value" : 3.0,
                      "description" : "avgdl, average length of field",
                      "details" : [ ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      },
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        },
        "_explanation" : {
                  ......
                }
              ]
            }
          ]
        }
      }
    ]
  }
}

# 利用 constant score filter 
POST my-index-000001/_search
{
  "explain": true,
  "query": {
    "constant_score": {
      "filter": {
        "term": {
          "full_text": "foxes"
        }
      }
    }
  }
}
# 相关返回
{
  ......
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "full_text" : "quick brown foxes!"
        },
        "_explanation" : {
          "value" : 1.0,
          "description" : "ConstantScore(full_text:foxes)",
          "details" : [ ]
        }
      },
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        },
        "_explanation" : {
          "value" : 1.0,
          "description" : "ConstantScore(full_text:foxes)",
          "details" : [ ]
        }
      }
    ]
  }
}

```
可以明显看到在 \_explanation 中，使用constant score 进行 filter 的算分逻辑明显简单了很多。  
#### 3.2 terms
terms 查询与 term 查询其实是一样的，只是 terms 查询可以对一个字段同时查询多个词项。  
相关使用如下：
```
GET /_search
{
  "query": {
    "terms": {
      "user.id": [ "kimchy", "elkbee" ], # 此处查询的词项可以是多个
      "boost": 1.0
    }
  }
}
```
**terms lookup**  
Term lookup 是一种参照某个索引文档的某一字段内容去搜索拥有同样值的文档方法。它可以获取现有文档的字段值，然后 ES 使用这些值作为搜索词去 terms 搜索。  
它有两个使用限制：
1. 使用 terms lookup ，\_source 设置为 enabled(默认是开启的)。
2. 不能在跨集群搜索上进行 terms lookup。

使用方法：
```
GET _search?pretty
{
  "query": {
    "terms": {
        "color" : {
            "index" : "my-index-000001",# 被参照的索引
            "id" : "2", # 被参照的文档
            "path" : "color" # 被参照文档的具体字段
        }
    }
  }
}
```
其中 path 在一些对象字段或者 nested 字段中，可以以“field.subfield”的形式查询。  
实践：
```
# 创建 my-index-000001 并设置 color 字段属性为 keyword。
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "color": { "type": "keyword" }
    }
  }
}
# 创建测试文档，注意文档 3 与其它两个文档并没有内容交集。
PUT my-index-000001/_doc/1
{
  "color":   ["blue", "green"]
}
PUT my-index-000001/_doc/2
{
  "color":   "blue"
}
PUT my-index-000001/_doc/3
{
  "color":   "red"
}
# 测试，使用文档 2 进行 terms lookup 查询，理论上会将文档 1 和 2 返回。
GET my-index-000001/_search?pretty
{
  "query": {
    "terms": {
        "color" : {
            "index" : "my-index-000001",
            "id" : "2",
            "path" : "color"
        }
    }
  }
}
# 返回结果
{
  "took" : 24,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "color" : [
            "blue",
            "green"
          ]
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "color" : "blue"
        }
      }
    ]
  }
}
```
#### 3.3 terms_set
terms_set可以根据搜索的多个词项和最少匹配词项数（minimum_should_match），返回复合条件的词项。而这个最少词项数是根据脚本或者某个数字字段决定的，即参数 minimum_should_match_script 和 minimum_should_match_field。  
我们先看一下 **minimum_should_match_field** 的实践：
1. 先创建一个索引，包含 keyword 字段和 long 字段，其中 programming_languages 是用于被搜索的字段，required_matches 则是被 minimum_should_match_field 使用的字段。
```
PUT /job-candidates
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "programming_languages": {
        "type": "keyword"
      },
      "required_matches": {
        "type": "long"
      }
    }
  }
}
```
2. 插入相关数据
```
PUT /job-candidates/_doc/1?refresh
{
  "name": "Jane Smith",
  "programming_languages": [ "c++", "java" ],
  "required_matches": 2
}
PUT /job-candidates/_doc/2?refresh
{
  "name": "Jason Response",
  "programming_languages": [ "java", "php" ],
  "required_matches": 2
}
PUT /job-candidates/_doc/3?refresh
{
  "name": "Jack",
  "programming_languages": [ "python", "golang","java", "php","c++" ],
  "required_matches": 3
}
PUT /job-candidates/_doc/4?refresh
{
  "name": "Tom",
  "programming_languages": [ "python", "golang","java", "php" ],
  "required_matches": 3
}
```
3. 根据 required_matches 搜索匹配 "java"、 "php"、"c++" 这些词项的内容 
```
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_field": "required_matches"
      }
    }
  }
}
# 返回结果
{
  "took" : 26,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 3,
      "relation" : "eq"
    },
    "max_score" : 1.6116207,
    "hits" : [
      {
        "_index" : "job-candidates",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 1.6116207,
        "_source" : {
          "name" : "Jack",
          "programming_languages" : [
            "python",
            "golang",
            "java",
            "php",
            "c++"
          ],
          "required_matches" : 3
        }
      },
      {
        "_index" : "job-candidates",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.1140156,
        "_source" : {
          "name" : "Jane Smith",
          "programming_languages" : [
            "c++",
            "java"
          ],
          "required_matches" : 2
        }
      },
      {
        "_index" : "job-candidates",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.64459586,
        "_source" : {
          "name" : "Jason Response",
          "programming_languages" : [
            "java",
            "php"
          ],
          "required_matches" : 2
        }
      }
    ]
  }
}
```
根据实际情况，文档 1 和 2 匹配搜索词项的两个且与 required_matches 的要求一直，因此返回；而文档 3 全部匹配且 required_matches 的值为 3，因此也符合；而文档 4 只匹配两个词项，而 required_matches 的值为 3，因此不会命中返回。  
 
**minimum_should_match_script**则是使用脚本定义所需的最少匹配词项数。这对一些动态条件场景比较适合。  
```
# 使用用词项数和固定数值的最小值来定义最少匹配词项数
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_script": {
          "source": "Math.min(params.num_terms, 4)"
        },
        "boost": 1.0
      }
    }
  }
}

# 使用用词项数和 required_matches 的最小值来定义最少匹配词项数
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_script": {
          "source": "Math.min(params.num_terms, doc['required_matches'].value)"
        },
        "boost": 1.0
      }
    }
  }
}
```
#### 3.4 wildcard
Wildcard 可以用通配符模式进行 term 查询。通配符是匹配一个或多个字符的占位符。例如，\* 通配符匹配零个或多个字符。可以将通配符操作符与其他字符组合起来创建通配符模式。  
使用方法如下：
```
GET /_search
{
  "query": {
    "wildcard": {
      "user.id": {
        "value": "ki*y",
        "boost": 1.0,
        "rewrite": "constant_score"
      }
    }
  }
}
```
其中 ki\*y 可以匹配 kiy、kity 或者 kimchy。  
通配符模式主要有两种方式：
* ?，问号用于匹配任意单个字符。
* \*,星号用于匹配 0 个或者多个字符。

注意：
1. 避免以 \* 或 ? 开头。这可能会增加查找匹配项所需的迭代次数，并降低搜索性能。
2. search.allow_expensive_queries 如果设置为 false 的话，那么 Wildcard 查询将被限制。
#### 3.5 range
Range 查询可以用于查询符合给定数据或者词项范围条件的相关文档。  
使用方法如下：
```
GET /_search
{
  "query": {
    "range": {
      "age": {
        "gte": 10,
        "lte": 20
      }
    }
  }
}
```
这个 DSL 是用于查询 age 字段范围大于等于 10，小于等于 20 的文档数据。  
相关参数：
* 运算关系符:
  1. gt：大于
  2. gte：大于等于
  3. lt：小于
  4. lte：小于等于
* format：该参数主要针对 date 类型字段的格式设置，ES 会默认使用字段在 mapping 中的格式。
* time_zone：该参数也是对于 date 类型字段的设置。
* relation：这个参数主要与 range 类型的范围计算有关。
  1. INTERSECTS：默认值，只要查询的范围与文档有交集即可返回。
  2. CONTAINS：只有查询的范围被包含在文档的范围内才可返回，即查询的范围小于文档的范围。
  3. WITHIN:只有查询的范围包含文档的范围才可返回，即查询的范围大于文档的范围。

关于 range 的 relation 计算可以主要参考下面的实践：  
创建一个 range 类型的字段 agerange  
```
POST my-index-000001/_mappings
{
  "properties": {
    "agerange": {
      "type": "integer_range"
    }
  }
}
```  
创建两个文档  
```
PUT my-index-000001/_doc/11?refresh
{
  "agerange": {
    "lte": 21,
    "gte": 10
  }
}
PUT my-index-000001/_doc/12
{
  "agerange": {
    "lte": 30,
    "gte": 20
  }
}
```  
首先使用默认的 INTERSECTS 查询 10 到 21 范围的文档，应该两个文档都返回。  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 21
      }
    }
  }
}
# 返回内容
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "12",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 30,
            "gte" : 20
          }
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
再使用 CONTAINS 查询范围为 10 到 21 的文档，应该只返回文档 11.  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 21,
        "relation": "CONTAINS"
      }
    }
  }
}
# 返回内容
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
再使用 CONTAINS 查询范围为 20 到 21 的文档，应该两个文档都返回.  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 20,
        "lte": 21,
        "relation": "CONTAINS"
      }
    }
  }
}
# 返回内容
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "12",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 30,
            "gte" : 20
          }
        }
      }
    ]
  }
}
```
我们再使用 WITHIN 查询范围为 10 到 22 的文档，应该只返回文档 11.  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 21,
        "relation": "WITHIN"
      }
    }
  }
}
# 返回内容
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
最后使用 WITHIN 查询范围为 10 到 30 的文档，应该两个文档均返回。  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 30,
        "relation": "WITHIN"
      }
    }
  }
}
# 返回内容
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "12",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 30,
            "gte" : 20
          }
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
注意： range 查询 text 或者 keyword 需要打开 search.allow_expensive_queries 设置。

#### 3.6 fuzzy
Fuzzy 查询即模糊查询，用于返回包含与搜索词相似的词的文档。  
先来理解一下编辑距离（edit distance）的概念。编辑距离是将一个词项转换为另一个词项所需的一个字符的更改数。比如： 
* 更改一个字符( box → fox )
* 删除一个字符( black → lack )
* 插入字符( sic → sick )
* 调换两个相邻字符( act → cat )

为了找到相似的词项，模糊查询在指定的编辑距离内创建一组所有可能的变体或扩展的搜索词项。然后查询返回每个展开的精确匹配。  
使用方法：
```
GET /_search
{
  "query": {
    "fuzzy": {
      "user.id": {
        "value": "ki"
      }
    }
  }
}
```
相关参数：
* fuzziness：允许匹配的最大编辑距离。可以是 0/1/2/AUTO
* max_expansions:创建的最大变体或者扩展词项数。默认为50。有效的控制这个参数可以降低性能损耗。
* prefix_length：在创建展开时保持不变的起始字符数。默认值为0。 注意，正常使用中大部分的字母拼写错误发生在词的结尾，而不是词的开始。 例如通过将 prefix_length 设置为 3 ，能够显著降低匹配的词项数量。 
* transpositions：指示编辑是否包括两个相邻字符的换位(ab→ba)。默认值为true。

高阶使用：
```
GET /_search
{
  "query": {
    "fuzzy": {
      "user.id": {
        "value": "ki",
        "fuzziness": "AUTO",
        "max_expansions": 50,
        "prefix_length": 0,
        "transpositions": true,
        "rewrite": "constant_score"
      }
    }
  }
}
```

注意： range 查询 text 或者 keyword 需要打开 search.allow_expensive_queries 设置。
#### 3.7 prefix
Prefix 查询用于对某一字段查询特定开头的词项。  
使用方法：  
```
GET /_search
{
  "query": {
    "prefix": {
      "user.id": {
        "value": "ki"
      }
    }
  }
}
```
这里代表查询 user.id 字段词项符合 ki 开头的文档。  
```
# 创建测试数据
PUT my-index-000001/_doc/21
{"user.id":"kind"}
PUT my-index-000001/_doc/22
{"user.id":"kimy"}
PUT my-index-000001/_doc/23
{"user.id":"ki1212121"}
# 查询 ki 开头的 user.id 文档
GET my-index-000001/_search
{
  "query": {
    "prefix": {
      "user.id": {
        "value": "ki"
      }
    }
  }
}
# 返回结果
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 3,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "21",
        "_score" : 1.0,
        "_source" : {
          "user.id" : "kind"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "22",
        "_score" : 1.0,
        "_source" : {
          "user.id" : "kimy"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "23",
        "_score" : 1.0,
        "_source" : {
          "user.id" : "ki1212121"
        }
      }
    ]
  }
}

```

注意：
* **加速查询**：可以在创建 mapping 时使用 index_prefixes 参数加速前缀查询。如果启用，ES 将索引前缀在 2 到 5 个字符到一个单独的字段。这让 ES 以更大的索引为代价更有效地运行前缀查询。
* 需要打开 search.allow_expensive_queries 设置

#### 3.8 regexp
Regexp 查询是使用正则表达式进行匹配的查询方式。关于正则匹配支持的操作，可以见相关详细列表：https://www.elastic.co/guide/en/elasticsearch/reference/7.14/regexp-syntax.html   
使用方法：  
```
GET /_search
{
  "query": {
    "regexp": {
      "user.id": {
        "value": "k.*y",
        "flags": "ALL",
        "case_insensitive": true,
        "max_determinized_states": 10000,
      }
    }
  }
}
```
以上查询代表使用正则表达式"k.\*y"去匹配相关符合条件的文档。    
相关参数：  
* flags：为正则表达式启用可选操作符。有关有效值和更多信息，请参见 https://www.elastic.co/guide/en/elasticsearch/reference/7.14/regexp-syntax.html 。  
* case_insensitive：当设置为true时，允许正则表达式值与索引字段值进行不区分大小写的匹配。Default为false，这意味着匹配的大小写敏感性取决于底层字段的映射。
* max_determinized_states：查询所需的匹配词项的最大数目。默认是 10000。 ES 在内部使用 Apache Lucene 解析正则表达式。 Lucene 将每个正则表达式转换为包含若干确定状态的有限词项。可以使用此参数防止转换无意中消耗过多资源。为了运行复杂的正则表达式，需要增加这个限制。  
#### 3.9 ids
Ids 查询是直接查询 \_id 字段的一种较为便捷的查询方式。  
使用方法：
```
GET /_search
{
  "query": {
    "ids" : {
      "values" : ["1", "4", "100"]
    }
  }
}
```
#### 3.10 exists
Exists 查询用于查询一个字段内容是否为空，若不为空则返回。Exists 可以与 must_not 布尔查询一起使用查找字段没有值的文档。    
由于下列原因，文档字段值可能不存在：
1. 源JSON中的字段为 null 或 \[\]
2. 该字段在mapping中有 “index”:false 设置
3. 字段值的长度超过了映射中的 ignore_above 设置
4. 该字段值是畸形的，并且在mapping中定义了 ignore_malformed

使用方法：
```
GET /_search
{
  "query": {
    "exists": {
      "field": "user"
    }
  }
}
```
**注意**：当出现下列情况时，字段还是存在的：
1. 空字符串，如"  "或"-"
2. 包含null和另一个值的数组，例如\[ null, "foo"\]
3. 在字段mapping中定义的自定义空值


### 4 全文搜索的相关方法
全文搜索的方法主要有：match/match_phrase/match_phrase_prefix/multi_match/match_bool_prefix/query_string/simple_query_string/intervals/combined_fields 九种方法。  
相比精确查询，全文搜索的查询方式相对会复杂很多。  
在全文搜索的复杂方法中，很多基于 match 查询的参数，如：analyzer, boost, operator, minimum_should_match, fuzziness, lenient, prefix_length, max_expansions, fuzzy_rewrite, zero_terms_query 和 cutoff_frequency 都能在其它方法中使用。  
#### 4.1 match
match 查询是一个经典的全文搜索方法，它会将查询的短语进行分词后对**某一字段**进行查询。match 查询对被分词后的 token 并没有强顺序关系，只要匹配就可以返回。  
使用方法：
```
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a test"
      }
    }
  }
}
```
参数：
* analyzer：设置将查询的短语转换成 token 的分词器。默认与字段索引时的分词器一致，即 mapping 设置的 analyzer 参数。如果没有设置，则使用默认的分词。
* auto_generate_synonyms_phrase_query：如果为 true ，为多项同义词自动生成短语查询。这个与分词器中设置的同义词相关。默认值为 true。
* fuzziness：允许匹配的最大编辑距离。可以是 0/1/2/AUTO
* max_expansions：创建的最大变体或者扩展词项数。默认为 50。
* prefix_length：在创建展开时保持不变的起始字符数。默认值为 0。
* fuzzy_transpositions：指编辑是否包括两个相邻字符的换位( ab → ba )。默认值为 true。
* lenient：如果为真，则忽略基于格式的错误，例如为数字字段提供 text 查询值。默认值为 false。
* operator：查询文本中的布尔逻辑。
    1. OR：默认值。例如，将查询值 “capital of Hungary” 解释为 “capital” 或者 “of” 或者 “Hungary”。
    2. AND：例如，将查询值 “capital of Hungary” 解释为 “capital” 和 “of” 和 “Hungary”。
* minimum_should_match：要返回的文档必须匹配的最小词项数。例如，“capital of Hungary” 被分词成 “capital”、“of”、“Hungary” 三个词项，minimum_should_match 设置为 2，则文档必须匹配前面三个词项中的两个才能返回。
* zero_terms_query：指示如果分析器删除所有词项时(例如使用停顿词分词器时)，是否不返回文档。
    1. none：默认值。如果分词器删除所有词项时，则不返回文档。
    2. all：与none相反，返回所有文档。

相关使用方法：
1. match 查询的 operator 和 minimum_should_match  
先创建一个测试索引和相关测试数据  
```
PUT my-index-000001

POST my-index-000001/_mapping
{
  "properties": {
    "message": {
      "type": "text"
    }
  }
}

PUT my-index-000001/_doc/1
{ "message":"this is test"}

PUT my-index-000001/_doc/2
{ "message":"this is a test again"}

PUT my-index-000001/_doc/3
{ "message":"this is  not a test"}
```
使用默认的分词器，可以看到几个文档会被解析成 "this"、"is"、"a"、"test"、"again"、"not" 这几个词项。  
```
POST _analyze
{
  "text": [
    "this is test",
    "this is a test again",
    "this is  not a test"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "test",
      "start_offset" : 8,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "this",
      "start_offset" : 13,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "is",
      "start_offset" : 18,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "a",
      "start_offset" : 21,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "test",
      "start_offset" : 23,
      "end_offset" : 27,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "again",
      "start_offset" : 28,
      "end_offset" : 33,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "this",
      "start_offset" : 34,
      "end_offset" : 38,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "is",
      "start_offset" : 39,
      "end_offset" : 41,
      "type" : "<ALPHANUM>",
      "position" : 9
    },
    {
      "token" : "not",
      "start_offset" : 43,
      "end_offset" : 46,
      "type" : "<ALPHANUM>",
      "position" : 10
    },
    {
      "token" : "a",
      "start_offset" : 47,
      "end_offset" : 48,
      "type" : "<ALPHANUM>",
      "position" : 11
    },
    {
      "token" : "test",
      "start_offset" : 49,
      "end_offset" : 53,
      "type" : "<ALPHANUM>",
      "position" : 12
    }
  ]
}

```
match 查询 "this is a test"，这个短语也会被分词成 "this"、"is"、"a"、"test"  
```
POST _analyze
{
  "text": [
    "this is a test"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "test",
      "start_offset" : 10,
      "end_offset" : 14,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}
```
如果按照 match 默认 or 的查询逻辑，只要有一个词项匹配就会返回，那么测试的三个文档将全部返回。  
```
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a test",
        "operator": "or"
      }
    }
  }
}
# 返回结果
{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 48,
    "successful" : 48,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 3,
      "relation" : "eq"
    },
    "max_score" : 0.5298672,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 0.5298672,
        "_source" : {
          "message" : "this is  not a test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.5298672,
        "_source" : {
          "message" : "this is a test again"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.30433932,
        "_source" : {
          "message" : "this is test"
        }
      }
    ]
  }
}
```
但是如果设置 minimum_should_match 为 4，则需要有四个词项匹配，那么只有文档 2 和 3 符合了。  
```
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a test",
        "operator": "or",
        "minimum_should_match": 4
      }
    }
  }
}
# 返回结果
{
  "took" : 10,
  "timed_out" : false,
  "_shards" : {
    "total" : 48,
    "successful" : 48,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.5298672,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 0.5298672,
        "_source" : {
          "message" : "this is  not a test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.5298672,
        "_source" : {
          "message" : "this is a test again"
        }
      }
    ]
  }
}
```
然后，再看一下 operator 为 and 的时候。其实，可以发现 and 的情况与之前设置 minimum_should_match 为 4 的查询一致，因为两者都代表查询时，每个词项都需要匹配上。    
```
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a test",
        "operator": "and"
      }
    }
  }
}
# 返回结果
{
  "took" : 15,
  "timed_out" : false,
  "_shards" : {
    "total" : 48,
    "successful" : 48,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.5298672,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 0.5298672,
        "_source" : {
          "message" : "this is  not a test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.5298672,
        "_source" : {
          "message" : "this is a test again"
        }
      }
    ]
  }
}
```


2. match 中的模糊查询  
fuzziness 的一系列参数可以使 match 解析出的词项进行模糊匹配。具体相关参数的使用方法与 fuzzy 查询一致，因此不详细展开了。  
来看下面的例子：
```
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a testt",
        "fuzziness": "AUTO"
        , "operator": "and"
      }
    }
  }
}
# 返回结果
{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 48,
    "successful" : 48,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.50886154,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 0.50886154,
        "_source" : {
          "message" : "this is  not a test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.50886154,
        "_source" : {
          "message" : "this is a test again"
        }
      }
    ]
  }
}

```
很明显，虽然查询的内容中错误的把“test”写成了“testt”，但是经过 fuzziness 参数的调整，达到了纠错的效果。  


#### 4.2 match_bool_prefix 
match_bool_prefix 查询会将被查询的内容根据分词器解析成词项进行单独的查询。除最后一个词外的每个词项都是用 term 查询。最后一个词项用于 prefix 查询。  
比如下面这个查询：
```
GET /_search
{
  "query": {
    "match_bool_prefix" : {
      "message" : "quick brown f"
    }
  }
}
```
这里分词器会产生 "quick"、"brown"、"f" 三个词项，然后组成与下面效果一致的 bool 查询  
```
GET /_search
{
  "query": {
    "bool" : {
      "should": [
        { "term": { "message": "quick" }},
        { "term": { "message": "brown" }},
        { "prefix": { "message": "f"}}
      ]
    }
  }
}
```
match_bool_prefix 在使用的时候有以下一些注意点：
1. analyzer 的设置对搜索词项的解析非常重要，最后一个词项的 prefix 查询是否符合需求和 analyzer 很有关系。
2. match_bool_prefix 查询支持 match 查询中 minimum_should_match 和 operater 参数，这次参数会应用到解析的子查询中（bool 后的 term 查询）。在大多数情况下，构造的 bool 查询中的子句数量和被搜索短语解析出的词项数量一致。
3. fuzziness、prefix_length、max_expansions、fuzzy_transpositions 和fuzzy_rewrit e参数可以应用到为除了最后一项之外的所有项构造的 term 子查询。它们对为最后一个词构造的 prefix 查询没有任何影响。

#### 4.3 match_phrase
match_phrase 执行的是**短语查询**，与简单的 match 查询不同的是，match_phrase 在经过 analyzer 解析后保持了词项的匹配顺序。因此实现了短语被完整匹配的场景。  
比如，关于 "foxes brown" 的查询：  
先测试 match 查询
```
GET my-index-000001/_search
{
  "query": {
    "match": {
      "full_text": "foxes brown"
    }
  }
}
# 返回结果
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.26706278,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.26706278,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.26706278,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
两个结果都返回了，查询出的结果与词项之间的排序并没有关联。
再使用 match_phrase 查询。    
```
GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "full_text": "foxes brown"
    }
  }
}
# 返回结果
{
  "took" : 11,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.26706278,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.26706278,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
只有文档 2 返回了，返回文档的词项顺序与被查询的文档词项顺序一致。  
match_phrase 查询为什么能将查询的内容保持原来的顺序呢?   
这主要依靠 **slop 参数**。 slop 参数默认值为0， slop 主要设置词项间的匹配距离，将顺序匹配词项并符合匹配距离的文档返回。
查询有这三个条件：
1. **按照词项顺序**查询文档。
2. 检查被匹配到的文档中词项之间的**距离总和是否在 slop 参数设置的匹配距离**内。
比如下面几个文档：
1. this is test
2. this is a test
3. this is not a test
4. this a is  not a test
5. this a  or is  not a test 

我们确认下词项间的距离，可以通过 \_analyze API 查看 position 来计算。为了简洁，我们看下第一个和最后一个文档。
```
GET _analyze
{
  "text": [
    "this is test"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "test",
      "start_offset" : 8,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}

GET _analyze
{
  "text": [
    "this a  or is  not a test"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "a",
      "start_offset" : 5,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "or",
      "start_offset" : 8,
      "end_offset" : 10,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "is",
      "start_offset" : 11,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "not",
      "start_offset" : 15,
      "end_offset" : 18,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "a",
      "start_offset" : 19,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "test",
      "start_offset" : 21,
      "end_offset" : 25,
      "type" : "<ALPHANUM>",
      "position" : 6
    }
  ]
}
```
可以看到，position基本就是单词项的顺序。  
现在进行 match_phrase 查询 "this test"。  
```
GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "message": {
        "query": "this is test",
        "slop": 0
      }
    }
  }
}
# 返回结果只有文档 1
{
  ······
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.26582208,
        "_source" : {
          "message" : "this is test"
        }
      }
    ]
  }
}

# 将 slop 设置为 1
GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "message": {
        "query": "this is test",
        "slop": 1
      }
    }
  }
}
# 返回文档 1 和 2
{
  ······
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.26582208,
        "_source" : {
          "message" : "this is test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.16089231,
        "_source" : {
          "message" : "this is  a test"
        }
      }
    ]
  }
}
# 将 slop 参数设置为4 
GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "message": {
        "query": "this is test",
        "slop": 4
      }
    }
  }
}
# 返回了全部 5 个文档
{
  ······
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.26582208,
        "_source" : {
          "message" : "this is test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.16089231,
        "_source" : {
          "message" : "this is  a test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 0.106328845,
        "_source" : {
          "message" : "this is  not a test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "5",
        "_score" : 0.07501727,
        "_source" : {
          "message" : "this a is  not a test"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "4",
        "_score" : 0.055580944,
        "_source" : {
          "message" : "this a  or is  not a test"
        }
      }
    ]
  }
}

```

注意：如果slop的值足够大，那么返回的文档中词项顺序会有一种无序的现象。  
比如对 "foxes brown" 这么查询：
```
GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "full_text": {
        "query": "foxes brown",
        "slop": 2
      }
    }
  }
}
# 返回结果
{
  ······
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.17439455,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      }
    ]
  }
}
```
这是因为在词项 foxes 之后两个位置的计算中，foxes 处于位置 3 ，那么在位置 5 之前的词项内出现 brown 文档就符合条件。但是文档 1 只有三个位置，那么想要从位置 3 再增加，只能从头开始，也就是位置2之前的词项如果有 brown 也是符合条件的。因此会造成文档 1 也被返回，match_phrase 查询结果并没有按照词项顺序匹配的表象。

#### 4.4 match_phrase_prefix
match_phrase_prefix 查询会将被查询的内容进行分词后进行 match_phrase 查询，其中最后一个词项在做词项比对的时候是 prefix 查询。  
比如下面三个文档：
1. quick brown fox
2. two quick brown ferrets
3. the fox is quick and brown

进行下面的查询：
```
GET my-index-000001/_search
{
  "query": {
    "match_phrase_prefix": {
      "message": {
        "query": "quick brown f"
      }
    }
  }
}
```
则能正常返回前两个文档。  
#### 4.5 multi_match
multi_match 查询，即多字段查询，区别于 term 和 match 对单一字段的查询，multi_match 扩展了搜索的范围。
使用方式如下：
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":    "this is a test", 
      "fields": [ "subject", "message" ] 
    }
  }
}
```
在 fields 内容中，使用者可以指定多个字段查询。  
其中：
1. 可以使用 \* 代替字符进行 wildcard 匹配。比如： `"fields": [ "title", "*_name" ] `代表 fisrt_name 和 last_name。**注意**，在使用 wildcard 匹配时，匹配到的字段数如果过多，则最大能查询的条件数受 indices.query.bool.max_clause_count 的限制，默认是1024。
2. 可以使用 \^ 符号进行字段查询时的加权。比如：`"fields" : [ "subject^3", "message" ]`,其中 subject 字段在算分时时 message 字段的三倍。

在多字段查询时，字段和被查询词项的关系主要有：best_fields/most_fields/cross_fields/phrase/phrase_prefix/bool_prefix，这六种关联方式。  
* **best_fields**：默认的查询方式，返回的文档排序按照其中某个字段得分最高的排在前面，这是一种单字段匹配优先的查询方式。  
比如：查询 "brown fox",单个字段中出现 "brown fox" 的文档要比两个字段分别出现 brown 和 fox 的文档得分高。  
best_fields 的方式是产生一个多 match 查询，然后将**多个字段查询条件进行 dis_max 查询，将单字段得分高的排在前面**。  
类似于这样的查询：  
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "brown fox",
      "type":       "best_fields",
      "fields":     [ "subject", "message" ],
      "tie_breaker": 0.3
    }
  }
}
```
执行起来是这样的：
```
GET /_search
{
  "query": {
    "dis_max": {
      "queries": [
        { "match": { "subject": "brown fox" }},
        { "match": { "message": "brown fox" }}
      ],
      "tie_breaker": 0.3
    }
  }
}
```
在查询中算分过程如下如下：
1. 取字段算分中的最高分；
2. 加上 tie_breaker 乘以其它匹配字段的 \_score 

* **most_fields**：返回的文档排序按照被查询字段匹配的越多排在越前面。  
most_fields 查询方法如下：
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "quick brown fox",
      "type":       "most_fields",
      "fields":     [ "title", "title.original", "title.shingles" ]
    }
  }
}
```
等同于
```
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "title":          "quick brown fox" }},
        { "match": { "title.original": "quick brown fox" }},
        { "match": { "title.shingles": "quick brown fox" }}
      ]
    }
  }
}
```
**最后算分将每个 match 子句的得分加在一起，然后除以 match 子句的数量。**  

**注意**：operator 和 minimum_should_match 条件是对 best_fields 和 most_fields 查询产生的每个查询子句的查询关系，而不会对查询字段进行 operator 的控制。  
比如：
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "Will Smith",
      "type":       "best_fields",
      "fields":     [ "first_name", "last_name" ],
      "operator":   "and" 
    }
  }
}
```

执行的效果则是 (+first_name:will +first_name:smith) | (+last_name:will  +last_name:smith) 的效果，即每个 match 查询的 and 关系。  
* **phrase和phrase_prefix**：在结果排序上的逻辑与 best_fields 一致,只是在查询方式上以 match_phrase 或者 match_phrase_prefix 的方式进行。  
即，下面的查询：
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "quick brown f",
      "type":       "phrase_prefix",
      "fields":     [ "subject", "message" ]
    }
  }
}
```
与这个查询是一样的   
```
GET /_search
{
  "query": {
    "dis_max": {
      "queries": [
        { "match_phrase_prefix": { "subject": "quick brown f" }},
        { "match_phrase_prefix": { "message": "quick brown f" }}
      ]
    }
  }
}
```
* **bool_prefix**：在结果排序的逻辑与 most_fields 一致，但是查询使用的是 match_bool_prefix 而不是 match。
即，下面的查询：
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "quick brown f",
      "type":       "bool_prefix",
      "fields":     [ "subject", "message" ]
    }
  }
}
```
与这个查询类似   
```
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match_phrase_prefix": { "subject": "quick brown f" }},
        { "match_phrase_prefix": { "message": "quick brown f" }} 
      ]
    }
  }
}
```
最后算分将每个 match 子句的得分加在一起，然后除以 match 子句的数量。  
* **cross_fields**：将被查询的字段是为一个大字段，进行统一的分词并按照要求进行查询。  
cross_fields 主要可以在查询时指定字段之间的关系，可以对词项在字段间的查询有更多的操控性，而不是统一默认的关系，甚至每个字段也并不一定需要满足才能返回文档。  
比如下面的查询：
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "Will Smith",
      "type":       "cross_fields",
      "fields":     [ "first_name", "last_name" ],
      "operator":   "and"
    }
  }
}
```
查询逻辑更像是这样： (first_name:will OR last_name:will) + (first_name:smith OR last_name:smith)，而两个字段并不一定都需要匹配 will 或者 smith，两个字段中有一个能匹配上文档就可以返回。  
由于在 TF/IDF 算法中，词频是一个重要的影响因素，因此 cross_fields 能有有效消除词频在不同字段中的差距，解决了 best_fields 的词频差问题。  
注意，cross_fields 通常只对 boost 为 1 的短字符串字段有用。除此之外，索引算法中词频和长度标准化也会对分数产生影响，从而导致 term 统计数据的混合不再有意义。

cross_fields 在分词器的处理上，只会将分词器相同的字段分组查询，然后对分组进行最高分比较。    
比如，first 和 last 字段是相同的分词器，而 first.simple 和 last.simple 使用的是相同的分词器。
```
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "Jon",
      "type":       "cross_fields",
      "fields":     [
        "first", "first.simple",
        "last",  "last.simple"
      ]
    }
  }
}
```
那么查询逻辑是这样的  
```
GET /_search
{
  "query": {
    "dis_max": {
      "queries": [
        {
          "multi_match" : {
            "query":      "Jon",
            "type":       "cross_fields",
            "fields":     [ "first", "last" ] 
          }
        },
        {
          "multi_match" : {
            "query":      "Jon",
            "type":       "cross_fields",
            "fields":     [ "first.simple","last.simple" ]
          }
        }
      ]
    }
  }
}
```

**关于 minimum_should_match**：在多分词器分组的条件下，minimum_should_match 对分组的查询是对每个查询子项的统一化设置（即与 best_fields 一样的情况），所以有特殊的条件可以考虑分开重写。  
注意：fuzziness 参数不可用于 cross_fields、phrase和phrase_prefix 类型，  

#### 4.6 query string
Query string 查询是一种便捷的语法查询方式，解析器根据提供的查询字符穿进行严格的语法解析，最后返回相应的文档。  
主要有这些特点：
1. 严格的语法解析，对查询内容的正确性要求较高。也因此 ES 官方不建议在搜索框使用 query_string 查询。
2. 操作符的使用使得查询语句更加简洁，且逻辑更加清晰。
3. 允许使用通配符、正则表达式等方法。
4. 基本覆盖了全文搜索的绝大数功能。

使用方法：  
```
GET /_search
{
  "query": {
    "query_string": {
      "query": "(new york city) OR (big apple)",
      "default_field": "content"
    }
  }
}
```
这个查询等同于下面的查询。  
```

GET my-index-000002/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "content": "new york city"
          }
        },
        {
          "match": {
            "content": "big apple"
          }
        }
      ]
    }
  }
}
```
query_string 的主要参数都在之前的篇幅中都解释过，因此不在这里赘述了。  
我们主要看一下 query_string 的语法运用规则：
1. 字段查询：默认的查询方式是`status:active`这样的方式。可以在字段中添加操作符，比如`title:(quick OR brown)`;也可以对字段使用匹配，`book.\*:(quick OR brown)`,注意转义符的使用；对被查询的文档是使用**英文双引号**可以达到 match_phrase 的效果，`author:"John Smith"`; exists 方法的使用则是这样`_exists_:title`
2. wildcards方法：匹配符的方法也是 `?` 代表一个字符， `*` 代表多个字符。
3. 正则表达式：正则表达式模式可以嵌入到查询字符串中，方法是将用斜杠`/`括起来,比如：`name:/joh?n(ath[oa]n)/`.
4. 模糊查询：模糊查询的符号是 `~`,比如：`quikc~ brwn~ foks~`;可以在 ~ 后面添加编辑距离参数，比如：`quikc~2`.注意，模糊查询是针对 term 查询时词项的模糊匹配。
5. 相似查询（proximity search）：与模糊查询的方式操作符号一样也是`~ `，但是是对 match_phrase 查询的两个词项间的距离编辑，比如：`"fox quick"~5` 
6. 范围查询：中阔号是两边范围包含，比如`count:[1 TO 5]`是 count 大于等于 1 小于等于 5 ；花括号是两边范围不包含，比如`count:{1 TO 5}`是 count 大于 1 小于 5 ；如果是单纯的大于或者小于则用 * 号代表无范围限制，比如`count:[1 TO *]`代表 count 大于等于 1，`count:{* TO 5}`代表小于5；当然两个符号也可以组合，比如`count:{1 TO 5]`代表大于 1 小于等于 5. 
7. 加权：加权主要使用的符号是`^`，比如`quick^2 fox`代表命中 quick 的文档算分是 fox 的 2 倍。
8. 布尔运算：默认空格是或查询`+`号是 must ，`-`号是 must not。比如`quick brown +fox -news`是查询必须有 fox ，不包含 news ，包含 quick brown 其中一项的文档。同时布尔运算符包含 `AND OR NOT` 和 `&& ||  !` 两种形式的表达式。
9. 分组：括号用于表示分组，比如 `(quick OR brown) AND fox` 或者 `status:(active OR pending) title:(full text search)^2`
10. 多字段查询：多字段查询的字段间关系默认是 OR 的关系.
比如下面的查询：
```
GET /_search
{
  "query": {
    "query_string": {
      "fields": [ "content", "name" ],
      "query": "this AND that"
    }
  }
}
```
等同于
```
GET /_search
{
  "query": {
    "query_string": {
      "query": "(content:this OR name:this) AND (content:that OR name:that)"
    }
  }
}
```
同时，multi_match 中的各种查询类型也可以通过 type 参数实现。

注意：
1. 在 query_string 中避免查询 nested 的文档。  
2. query_string 查询可以在内部转换为 prefix 查询，如果 ES 禁用 prefix 查询，query_string 将不会执行查询并抛出异常。
3. 关于转义符，在 json 中转义符是 "\\",比如`"query" : "kimchy\\!"` 
4. 在多字段查询中，由于字段和词项之间的布尔运算规则的交叉，会导致运算逻辑相对晦涩不明。因此如果有多字段查询最佳的方式是通过分组将其查询逻辑清晰化，避免查询失败。
#### 4.7 simple_query_string
simple_query_string 是一种相对简化的查询语法，虽然比起 query_string 的完善功能 simple_query_string 的使用有诸多限制，但是 simple_query_string 查询不会因无效语法返回错误。相反，它会忽略查询字符串中任何无效的部分。  
使用方法：
```
GET /_search
{
  "query": {
    "simple_query_string" : {
        "query": "\"fried eggs\" +(eggplant | potato) -frittata",
        "fields": ["title^5", "body"],
        "default_operator": "and"
    }
  }
}
```
simple_query_string 的主要参数都在之前的篇幅中都解释过，因此不在这里赘述了。
* 使用语法
语法操作符主要作用于查询词项，有以下这些：
1. `+` 代表 and 关系， `|` 代表 or ,`-`代表 not。
2. `""` 英文双引号使用 match_phrase。
3. `*`是通配符。
4. `()`是分组。
5. `~N`是模糊查询或者相似查询的字符或者词项编辑距离。

* limit operators
在 simple_query_string 查询中可以使用 flags 参数进行对查询语法的限制，对各种操作方法使用`|`隔开。  
比如：
```
GET /_search
{
  "query": {
    "simple_query_string": {
      "query": "foo | bar + baz*",
      "flags": "OR|AND|PREFIX"
    }
  }
}
```
这个查询中，限定了 OR AND PREFIX 三种语法，其余的语法运算符都会被限制掉。  
flag 的参数值：ALL(默认)/AND/ESCAPE/FUZZY/NEAR/NONE/NOT/OR/PHRASE/PRECEDENCE/PREFIX/SLOP/WHITESPACE  

* 字段通配符和加权
在查询字段上，也可以使用通配符`*`，比如：  
```
GET /_search
{
  "query": {
    "simple_query_string" : {
      "query":    "Will Smith",
      "fields": [ "title", "*_name" ] 
    }
  }
}
```
同时也可以对字段进行加权，使用符号`^`：  
```
GET /_search
{
  "query": {
    "simple_query_string" : {
      "query" : "this is a test",
      "fields" : [ "subject^3", "message" ] 
    }
  }
}
```

#### 4.8 intervals
这是一种根据匹配的 term 的顺序和相似度返回文档的一种方法。  
intervals 查询根据一些定义的匹配规则，来查询某个字段与词项一致的 tokens 是否符合规则组合排序。主要根据词项的顺序和间隔来组装规则。    
使用方法：
```
POST _search
{
  "query": {
    "intervals" : {
      "my_text" : {
        "all_of" : {
          "ordered" : true,
          "intervals" : [
            {
              "match" : {
                "query" : "my favorite food",
                "max_gaps" : 0,
                "ordered" : true
              }
            },
            {
              "any_of" : {
                "intervals" : [
                  { "match" : { "query" : "hot water" } },
                  { "match" : { "query" : "cold porridge" } }
                ]
              }
            }
          ]
        }
      }
    }
  }
}
```
上面的查询要求在 my_text 字段先无间隔按照词项顺序匹配出 my favorite food 的文档，再去查询符合 hot water 或者 cold porridge 的文档。  
intervals方法对各个查询场景的类型如下：
* match：普通的 match 场景。
* prefix：前缀匹配查询。
* wildcard：使用贪婪匹配查询。
* fuzzy：模糊匹配查询。
* all_of：返回全部满足子查询的条件的文档。
* any_of：返回任意满足一个子查询条件的文档。
* filter:为其它 intervals 查询添加规则条件。

在 intervals 方法中主要有下面几个特定参数可以选择：
1. max_gaps:词项间最大的位置距离，比这个距离更远的词项则认为不匹配，默认是 -1。设置为 -1 则是没有距离限制，设置为 0 则是词项必须相邻出现。用于 match、all_of 类型。
2. ordered: 默认 false，如果为 true，则词项必须按照顺序出现。用于 match、all_of 类型。

对于 filter 类型主要是相关条件的关系判断，有 after/before/contained_by/containing/not_contained_by/not_containing/not_overlapping/overlapping/script.  

**查询字段的数量限制**：在 combined_fields 查询中，这个查询字段的限制受制于查询逻辑中产生的组合字段与词项匹配出的子查询数量。    
与 multi_match 的比较：  
1. 在查询效果上 combined_fields 与 cross_fields 的效果是一致的。
2. combined_fields 只能查询相同分词器的字段，如果有不同的分词器
### 5 lucene中的精确搜索和全文匹配
* token的要素
* tokenization
* parser
#### 5.1 lucene的查询基本类
#### 5.2 lucene的queryparser
### 6 思考和总结
