精确搜索和全文搜索是使用 ES 进行搜索时基本且常见的一个话题。  
精确搜索往往有着**查询逻辑简单、查询效率较高、方便理解**的优势，但也对内容的搜索方式限制较大，**一般只能使用一个词对某一个字段进行搜索**，且搜索的结果是否命中与文档索引时分词器的设置或者选择关联较大。  
而全文搜索则与之相反，内容搜索的方式**灵活且多样**，可以**不同的内容在 不同的字段同时进行搜索**，甚至能根据复杂的场景制定不同的搜索方法以满足不同的查询需求，**相比精确搜索更容易搜索出想要的结果**。同时也存在，查询逻辑复杂、运用起来不易理解、搜索效率相对较低的缺点。  
下面我们来比对一下精确搜索最常见的使用方法 term 与全文搜索最常见的使用方法 match 之间的差别。
### 1 term 与 match  
```
#先创建一个测试索引并放入两条数据
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_text": { "type": "text" }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "full_text":   "quick brown foxes!"
}

PUT my-index-000001/_doc/2
{
  "full_text":   "Quick Foxes Brown !"
}
```
我们先使用 term 查询搜索一下关键词 foxes 。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "foxes"
    }
  }
}
```
返回结果为
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
可以发现两条记录都命中了。我们再试试使用 foxes 后添加一个感叹号形成"foxes!"搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "foxes!"
    }
  }
}
```
返回结果为空
```
{
  ···
    "hits" : [ ]
  }
}

```
我们再使用"brown foxes"这两个关键词试验一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "brown foxes"
    }
  }
}
```
会发现返回结果也是为空。  
```
{
  ···
    "hits" : [ ]
  }
}

```
上述三种查询关键词，只有 foxes 顺利能搜索返回相关文档，其它稍有变化的关键词或者组合的关键词都没有结果返回。  
下面我们看看 match。先使用 foxes 搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes"
    }
  }
}
```
正常返回两条结果。
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
再使用"foxes!"搜索一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes!"
    }
  }
}
```
也正常返回两条结果（节省篇幅此处不展示）。  
再使用两个关键词组合的"brown foxes"进行搜索。  
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "brown foxes"
    }
  }
}
```
这里还是返回两条结果，而不是按照两个关键词顺序更匹配的文档 1 。  
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
最后我们将两个关键词顺序颠倒一下。
```
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "foxes brown "
    }
  }
}
```
依旧返回两条结果，并且连顺序都一样。 
```
{
  ···
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "quick brown foxes!"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.36464313,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        }
      }
    ]
  }
}
```
从上面的例子可以窥探出精确搜索和全文搜索的一些特点：精确搜索一旦对搜索关键词把控不好就很有可能得不到搜索结果；而全文搜索虽然容易查出结果，但也往往会缺乏精度，返回过多的结果。  
那么，大家是否会有下面的问题：
1. 精确搜索究竟怎么查询才能匹配出更多的结果？
2. 全文搜索中有什么办法可以把搜索结果范围缩小？或者搜索出来的结果可以按照更高的匹配度排序？
3. 这两者查询背后究竟有什么样的原理？

我们进入下一环节。
### 2.1 原理
在解析精确搜索和全文搜索之前，我们先来理解两个在 ES 搜索查询中需要知道的两个概念 term 和 token 。  
**Term** 代表文本中的一个单词。这是搜索单元，即搜索的最小元素(可以称之为词项)。  
**Token** 文档进行索引的后最小元素，即语汇单元。Token 一般由项（term）、位置（position）、偏移量（offset）、类型（type）、标志位（flags）和有效负载这些要素组成。    
Token 的产生其实也是倒排索引产生的过程，是通过一定的解析规则将一段文档解析成一堆不可再拆分的词干的过程。  
而 ES 搜索的最终过程就是 term 与 token 进行比对的过程（我们在这里称它为词项比对）。  
我们可以通过 \_analyze API 体验到文档解析成 token 的过程。  
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "Quick Foxes Brown !"
  ]
}

#返回结果
{
  "tokens" : [
    {
      "token" : "quick",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "foxes",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "brown",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}

```
通过 API 可以看到，文档内容"Quick Brown Foxes!"经过 standard 分词器解析后形成 "quick","brown","foxes" 三个 token 。  
也可以看到每个 token 的 offset、type 和 position 的属性。  

```
https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/index/Term.html  
在 ES 中，Term 查询，对输⼊不做分词。会将输⼊作为⼀个整体，在倒排索引中查找准确的词项，并且使⽤相关度算分公式为每个包含该词项的⽂档进⾏相关度算分。  
term 查询在 text 文档中的实现精确查询，需要 term 与文档完全对应。  
https://www.zhihu.com/question/32133683  
https://lucene.apache.org/core/8_9_0/core/index.html
Plain text passed to Lucene for indexing goes through a process generally called tokenization. Tokenization is the process of breaking input text into small indexing elements – tokens. The way input text is broken into tokens heavily influences how people will then be able to search for that text. For instance, sentences beginnings and endings can be identified to provide for more accurate phrase and proximity searches (though sentence identification is not provided by Lucene).   
**tokenization**传递给Lucene用于索引的纯文本经过一个通常称为标记化的过程。标记化是将输入文本分解为小索引元素——标记的过程。输入文本被分割成符号的方式很大程度上影响了人们搜索该文本的方式。例如，可以识别句子的开头和结尾，以提供更准确的短语和邻近搜索(尽管Lucene不提供句子识别)。  
https://lucene.apache.org/core/8_9_0/core/index.html  
```
那我们先看看搜索与索引时的处理过程：
![索引与搜索](https://github.com/kin122/duoankin.github.io/blob/main/elasticsearch/%E6%90%9C%E7%B4%A2%E4%B8%8E%E7%B4%A2%E5%BC%95.png)   
注意：搜索和索引的过程中，分词器可以选择不一样的，虽然并不推荐这么做。  

现在我们可以看一下 ES 官方对全文搜索（full text queries）和精确搜索(term-level queries)的定义：
* 全文搜索：可以对多个字段同时搜索可被分词器解析的文本内容。 
* 精确搜索：将搜索内容作为精确值去查找文档，精确搜索不会对搜索内容进行分词器解析，并且只能对单个字段进行词项匹配。

两者之间的比对如下图：
![精确搜索与全文搜索](https://github.com/kin122/duoankin.github.io/blob/main/elasticsearch/%E7%B2%BE%E7%A1%AE%E6%90%9C%E7%B4%A2%E4%B8%8E%E5%85%A8%E6%96%87%E6%90%9C%E7%B4%A2.png)   

可以得出以下几点：
1. 精确搜索只能查询一个 field ，而全文搜索可以多个字段同时搜索
2. term 是将搜索内容直接作为一个最小单元与 token 的信息进行比对；全文搜索则是进行语法和分词器解析后，将解析到的 term 进行与文档索引后得到的 token 进行词项比对。
3. term 查询方式更加简单，确定了字段和比对的词项。而全文搜索中这两者可能会随着语法和分词器的变化而变化。
4. 从 lucene 源码实现的角度来说，term 查询的实现类是 TermQuery ，全文搜索使用的是 QueryParser 类，而 QueryParser 最终默认使用 TermQuery 进行多个 Term 的关系查询。

#### 2.2 实践
现在让我们重新看一下之前的 term 和 match 查询。  
首先看一下测试索引中的两个文档经过分词后的词项情况，由于之前文档 \_id 为2已经被解析过了，我们看一下文档 \_id 为1的情况：
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "quick brown foxes!"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "quick",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "brown",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "foxes",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}

```
可见三个 token 的词项内容是一样的，其它参数如 offset、position 等有了相应的变化。 
我们先看精确搜索的条件：  
第一次，term 查询了"foxes"，词项就为“foxes”，该词项存在于两个文档分词解析后的 token 中，所以可以两条都可返回。  
第二次，term 查询了"foxes!",词项为 “foxes！”，该词项与“foxes”并不完全一致，多了一个感叹号，所以命中为空。  
第三次，term 查询了"brown foxes"，词项为“brown foxes”，该词项并不与两个文档的任何token匹配，所以命中也为空。  
通过上面的分析，我们可以看出，**term 查询如果需要能正确匹配出想要的文档，那么查询的词项必须是与文档索引时分词出的 token 内容相一致**。  

现在我们看看 match 的全文搜索：  
第一次为 match 查询“foxes”，先使用 analyze API 查看下分词词根，注意之前索引文档的时候未指定索引分词器和搜索分词器，那么 ES 会采用默认的 standard 分词器。  
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "foxes"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "foxes",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    }
  ]
}
```
可以看出，分词器对搜索的内容没有做什么改变，因此两条文档全部命中也是很容易理解。  
第二次，match 查询“foxes!”，也使用 analyze API 查看下。  
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "foxes!"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "foxes",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    }
  ]
}
```
这次分词器把“foxes!”解析成“foxes”，因此两条文档也全部命中。  
我们再看看第三第四次 match 的内容：
```
POST _analyze
{
  "analyzer": "standard",
  "text": [
    "brown foxes"
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "brown",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "foxes",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}

POST _analyze
{
  "analyzer": "standard",
  "text": [
    "foxes brown "
  ]
}
# 返回结果
{
  "tokens" : [
    {
      "token" : "foxes",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "brown",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```
可见两个短语在被分词器解析后，两个词项和文档的 token 都能匹配，因此两条都能返回。  
由于 match 查询并没有词项的位置关系计算，因此两个搜索内容的效果是一样的，如果想要对全文搜索内容做更加位置关系的计算让其看上去更匹配，则可以使用 match_phrase，后面讲解全文搜索具体方法时会有相关详细解释。  

### 3 精确搜索的相关方法
精确搜索主要有 term/terms/terms_set/wildcard/range/fuzzy/prefix/regexp/ids/exists 十个主要查询方法。  
#### 3.1 term
term 查询是精确搜索最常见最基础的一种方式。  
日常用法如下：
```
GET /_search
{
  "query": {
    "term": {
      "user.id": { # 字段名
        "value": "kimchy", # 被搜索的词项
        "boost": 1.0
      }
    }
  }
}
```
主要参数：  
* boost：用于减少或增加查询的相关分数的浮点数值。默认为1.0。可以使用boost参数来调整包含两个或多个查询的搜索的相关性分数。Boost值相对于默认值1.0。0到1.0之间的升压值会降低相关性得分。大于1.0的值会增加相关分数.


使用建议：可以通过 Constant Score 将查询转换成⼀个 Filter，避免算分，并利⽤缓存，提⾼性能。  
```
# 原查询
GET my-index-000001/_search
{"explain": true,  # 打开 explain 查看算分逻辑
  "query": {
    "term": {
      "full_text": "foxes"
    }
  }
}
# 相关返回
{
  ......
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.18232156,
    "hits" : [
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "quick brown foxes!"
        },
        "_explanation" : {
          "value" : 0.18232156,
          "description" : "weight(full_text:foxes in 0) [PerFieldSimilarity], result of:",
          "details" : [
            {
              "value" : 0.18232156,
              "description" : "score(freq=1.0), computed as boost * idf * tf from:",
              "details" : [
                {
                  "value" : 2.2,
                  "description" : "boost",
                  "details" : [ ]
                },
                {
                  "value" : 0.18232156,
                  "description" : "idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:",
                  "details" : [
                    {
                      "value" : 2,
                      "description" : "n, number of documents containing term",
                      "details" : [ ]
                    },
                    {
                      "value" : 2,
                      "description" : "N, total number of documents with field",
                      "details" : [ ]
                    }
                  ]
                },
                {
                  "value" : 0.45454544,
                  "description" : "tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:",
                  "details" : [
                    {
                      "value" : 1.0,
                      "description" : "freq, occurrences of term within document",
                      "details" : [ ]
                    },
                    {
                      "value" : 1.2,
                      "description" : "k1, term saturation parameter",
                      "details" : [ ]
                    },
                    {
                      "value" : 0.75,
                      "description" : "b, length normalization parameter",
                      "details" : [ ]
                    },
                    {
                      "value" : 3.0,
                      "description" : "dl, length of field",
                      "details" : [ ]
                    },
                    {
                      "value" : 3.0,
                      "description" : "avgdl, average length of field",
                      "details" : [ ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      },
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        },
        "_explanation" : {
                  ......
                }
              ]
            }
          ]
        }
      }
    ]
  }
}

# 利用 constant score filter 
POST my-index-000001/_search
{
  "explain": true,
  "query": {
    "constant_score": {
      "filter": {
        "term": {
          "full_text": "foxes"
        }
      }
    }
  }
}
# 相关返回
{
  ......
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "full_text" : "quick brown foxes!"
        },
        "_explanation" : {
          "value" : 1.0,
          "description" : "ConstantScore(full_text:foxes)",
          "details" : [ ]
        }
      },
      {
        "_shard" : "[my-index-000001][0]",
        "_node" : "egEhfXrCTrycdbAwEy9z3Q",
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "full_text" : "Quick Foxes Brown !"
        },
        "_explanation" : {
          "value" : 1.0,
          "description" : "ConstantScore(full_text:foxes)",
          "details" : [ ]
        }
      }
    ]
  }
}

```
可以明显看到在 \_explanation 中，使用constant score 进行 filter 的算分逻辑明显简单了很多。  
#### 3.2 terms
terms 查询与 term 查询其实是一样的，只是 terms 查询可以对一个字段同时查询多个词项。  
相关使用如下：
```
GET /_search
{
  "query": {
    "terms": {
      "user.id": [ "kimchy", "elkbee" ], # 此处查询的词项可以是多个
      "boost": 1.0
    }
  }
}
```
**terms lookup**  
Term lookup 是一种参照某个索引文档的某一字段内容去搜索拥有同样值的文档方法。它可以获取现有文档的字段值，然后 ES 使用这些值作为搜索词去 terms 搜索。  
它有两个使用限制：
1. 使用 terms lookup ，\_source 设置为 enabled(默认是开启的)。
2. 不能在跨集群搜索上进行 terms lookup。

使用方法：
```
GET _search?pretty
{
  "query": {
    "terms": {
        "color" : {
            "index" : "my-index-000001",# 被参照的索引
            "id" : "2", # 被参照的文档
            "path" : "color" # 被参照文档的具体字段
        }
    }
  }
}
```
其中 path 在一些对象字段或者 nested 字段中，可以以“field.subfield”的形式查询。  
实践：
```
# 创建 my-index-000001 并设置 color 字段属性为 keyword。
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "color": { "type": "keyword" }
    }
  }
}
# 创建测试文档，注意文档 3 与其它两个文档并没有内容交集。
PUT my-index-000001/_doc/1
{
  "color":   ["blue", "green"]
}
PUT my-index-000001/_doc/2
{
  "color":   "blue"
}
PUT my-index-000001/_doc/3
{
  "color":   "red"
}
# 测试，使用文档 2 进行 terms lookup 查询，理论上会将文档 1 和 2 返回。
GET my-index-000001/_search?pretty
{
  "query": {
    "terms": {
        "color" : {
            "index" : "my-index-000001",
            "id" : "2",
            "path" : "color"
        }
    }
  }
}
# 返回结果
{
  "took" : 24,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "color" : [
            "blue",
            "green"
          ]
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "color" : "blue"
        }
      }
    ]
  }
}
```
#### 3.3 terms_set
terms_set可以根据搜索的多个词项和最少匹配词项数（minimum_should_match），返回复合条件的词项。而这个最少词项数是根据脚本或者某个数字字段决定的，即参数 minimum_should_match_script 和 minimum_should_match_field。  
我们先看一下 **minimum_should_match_field** 的实践：
1. 先创建一个索引，包含 keyword 字段和 long 字段，其中 programming_languages 是用于被搜索的字段，required_matches 则是被 minimum_should_match_field 使用的字段。
```
PUT /job-candidates
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "programming_languages": {
        "type": "keyword"
      },
      "required_matches": {
        "type": "long"
      }
    }
  }
}
```
2. 插入相关数据
```
PUT /job-candidates/_doc/1?refresh
{
  "name": "Jane Smith",
  "programming_languages": [ "c++", "java" ],
  "required_matches": 2
}
PUT /job-candidates/_doc/2?refresh
{
  "name": "Jason Response",
  "programming_languages": [ "java", "php" ],
  "required_matches": 2
}
PUT /job-candidates/_doc/3?refresh
{
  "name": "Jack",
  "programming_languages": [ "python", "golang","java", "php","c++" ],
  "required_matches": 3
}
PUT /job-candidates/_doc/4?refresh
{
  "name": "Tom",
  "programming_languages": [ "python", "golang","java", "php" ],
  "required_matches": 3
}
```
3. 根据 required_matches 搜索匹配 "java"、 "php"、"c++" 这些词项的内容 
```
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_field": "required_matches"
      }
    }
  }
}
# 返回结果
{
  "took" : 26,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 3,
      "relation" : "eq"
    },
    "max_score" : 1.6116207,
    "hits" : [
      {
        "_index" : "job-candidates",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 1.6116207,
        "_source" : {
          "name" : "Jack",
          "programming_languages" : [
            "python",
            "golang",
            "java",
            "php",
            "c++"
          ],
          "required_matches" : 3
        }
      },
      {
        "_index" : "job-candidates",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.1140156,
        "_source" : {
          "name" : "Jane Smith",
          "programming_languages" : [
            "c++",
            "java"
          ],
          "required_matches" : 2
        }
      },
      {
        "_index" : "job-candidates",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.64459586,
        "_source" : {
          "name" : "Jason Response",
          "programming_languages" : [
            "java",
            "php"
          ],
          "required_matches" : 2
        }
      }
    ]
  }
}
```
根据实际情况，文档 1 和 2 匹配搜索词项的两个且与 required_matches 的要求一直，因此返回；而文档 3 全部匹配且 required_matches 的值为 3，因此也符合；而文档 4 只匹配两个词项，而 required_matches 的值为 3，因此不会命中返回。  
 
**minimum_should_match_script**则是使用脚本定义所需的最少匹配词项数。这对一些动态条件场景比较适合。  
```
# 使用用词项数和固定数值的最小值来定义最少匹配词项数
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_script": {
          "source": "Math.min(params.num_terms, 4)"
        },
        "boost": 1.0
      }
    }
  }
}

# 使用用词项数和 required_matches 的最小值来定义最少匹配词项数
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_script": {
          "source": "Math.min(params.num_terms, doc['required_matches'].value)"
        },
        "boost": 1.0
      }
    }
  }
}
```
#### 3.4 wildcard
Wildcard 可以用通配符模式进行 term 查询。通配符是匹配一个或多个字符的占位符。例如，\* 通配符匹配零个或多个字符。可以将通配符操作符与其他字符组合起来创建通配符模式。  
使用方法如下：
```
GET /_search
{
  "query": {
    "wildcard": {
      "user.id": {
        "value": "ki*y",
        "boost": 1.0,
        "rewrite": "constant_score"
      }
    }
  }
}
```
其中 ki\*y 可以匹配 kiy、kity 或者 kimchy。  
通配符模式主要有两种方式：
* ?，问号用于匹配任意单个字符。
* \*,星号用于匹配 0 个或者多个字符。

注意：
1. 避免以 \* 或 ? 开头。这可能会增加查找匹配项所需的迭代次数，并降低搜索性能。
2. search.allow_expensive_queries 如果设置为 false 的话，那么 Wildcard 查询将被限制。
#### 3.5 range
Range 查询可以用于查询符合给定数据或者词项范围条件的相关文档。  
使用方法如下：
```
GET /_search
{
  "query": {
    "range": {
      "age": {
        "gte": 10,
        "lte": 20
      }
    }
  }
}
```
这个 DSL 是用于查询 age 字段范围大于等于 10，小于等于 20 的文档数据。  
相关参数：
* 运算关系符:
  1. gt：大于
  2. gte：大于等于
  3. lt：小于
  4. lte：小于等于
* format：该参数主要针对 date 类型字段的格式设置，ES 会默认使用字段在 mapping 中的格式。
* time_zone：该参数也是对于 date 类型字段的设置。
* relation：这个参数主要与 range 类型的范围计算有关。
  1. INTERSECTS：默认值，只要查询的范围与文档有交集即可返回。
  2. CONTAINS：只有查询的范围被包含在文档的范围内才可返回，即查询的范围小于文档的范围。
  3. WITHIN:只有查询的范围包含文档的范围才可返回，即查询的范围大于文档的范围。

关于 range 的 relation 计算可以主要参考下面的实践：  
创建一个 range 类型的字段 agerange  
```
POST my-index-000001/_mappings
{
  "properties": {
    "agerange": {
      "type": "integer_range"
    }
  }
}
```  
创建两个文档  
```
PUT my-index-000001/_doc/11?refresh
{
  "agerange": {
    "lte": 21,
    "gte": 10
  }
}
PUT my-index-000001/_doc/12
{
  "agerange": {
    "lte": 30,
    "gte": 20
  }
}
```  
首先使用默认的 INTERSECTS 查询 10 到 21 范围的文档，应该两个文档都返回。  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 21
      }
    }
  }
}
# 返回内容
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "12",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 30,
            "gte" : 20
          }
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
再使用 CONTAINS 查询范围为 10 到 21 的文档，应该只返回文档 11.  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 21,
        "relation": "CONTAINS"
      }
    }
  }
}
# 返回内容
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
再使用 CONTAINS 查询范围为 20 到 21 的文档，应该两个文档都返回.  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 20,
        "lte": 21,
        "relation": "CONTAINS"
      }
    }
  }
}
# 返回内容
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "12",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 30,
            "gte" : 20
          }
        }
      }
    ]
  }
}
```
我们再使用 WITHIN 查询范围为 10 到 22 的文档，应该只返回文档 11.  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 21,
        "relation": "WITHIN"
      }
    }
  }
}
# 返回内容
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
最后使用 WITHIN 查询范围为 10 到 30 的文档，应该两个文档均返回。  
```
POST my-index-000001/_search
{
  "query": {
    "range": {
      "agerange": {
        "gte": 10,
        "lte": 30,
        "relation": "WITHIN"
      }
    }
  }
}
# 返回内容
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "12",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 30,
            "gte" : 20
          }
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "11",
        "_score" : 1.0,
        "_source" : {
          "agerange" : {
            "lte" : 21,
            "gte" : 10
          }
        }
      }
    ]
  }
}
```
注意： range 查询 text 或者 keyword 需要打开 search.allow_expensive_queries 设置。

#### 3.6 fuzzy
参数：
* fuzziness
* max_expansions
* prefix_length
* transpositions
#### 3.7 prefix
Prefix 查询用于对某一字段查询特定开头的词项。  
使用方法：  
```
GET /_search
{
  "query": {
    "prefix": {
      "user.id": {
        "value": "ki"
      }
    }
  }
}
```
这里代表查询 user.id 字段词项符合 ki 开头的文档。  
```
# 创建测试数据
PUT my-index-000001/_doc/21
{"user.id":"kind"}
PUT my-index-000001/_doc/22
{"user.id":"kimy"}
PUT my-index-000001/_doc/23
{"user.id":"ki1212121"}
# 查询 ki 开头的 user.id 文档
GET my-index-000001/_search
{
  "query": {
    "prefix": {
      "user.id": {
        "value": "ki"
      }
    }
  }
}
# 返回结果
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 3,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "21",
        "_score" : 1.0,
        "_source" : {
          "user.id" : "kind"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "22",
        "_score" : 1.0,
        "_source" : {
          "user.id" : "kimy"
        }
      },
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "23",
        "_score" : 1.0,
        "_source" : {
          "user.id" : "ki1212121"
        }
      }
    ]
  }
}

```

注意：
* **加速查询**：可以在创建 mapping 时使用 index_prefixes 参数加速前缀查询。如果启用，ES 将索引前缀在 2 到 5 个字符到一个单独的字段。这让 ES 以更大的索引为代价更有效地运行前缀查询。
* 需要打开 search.allow_expensive_queries 设置

#### 3.8 regexp
Regexp 查询是使用正则表达式进行匹配的查询方式。关于正则匹配支持的操作，可以见相关详细列表：https://www.elastic.co/guide/en/elasticsearch/reference/7.14/regexp-syntax.html   
使用方法：  
```
GET /_search
{
  "query": {
    "regexp": {
      "user.id": {
        "value": "k.*y",
        "flags": "ALL",
        "case_insensitive": true,
        "max_determinized_states": 10000,
      }
    }
  }
}
```
以上查询代表使用正则表达式"k.\*y"去匹配相关符合条件的文档。    
相关参数：  
* flags：为正则表达式启用可选操作符。有关有效值和更多信息，请参见 https://www.elastic.co/guide/en/elasticsearch/reference/7.14/regexp-syntax.html 。  
* case_insensitive：当设置为true时，允许正则表达式值与索引字段值进行不区分大小写的匹配。Default为false，这意味着匹配的大小写敏感性取决于底层字段的映射。
* max_determinized_states：查询所需的匹配词项的最大数目。默认是 10000。 ES 在内部使用 Apache Lucene 解析正则表达式。 Lucene 将每个正则表达式转换为包含若干确定状态的有限词项。可以使用此参数防止转换无意中消耗过多资源。为了运行复杂的正则表达式，需要增加这个限制。  
#### 3.9 ids
Ids 查询是直接查询 \_id 字段的一种较为便捷的查询方式。  
使用方法：
```
GET /_search
{
  "query": {
    "ids" : {
      "values" : ["1", "4", "100"]
    }
  }
}
```
#### 3.10 exists

### 4 全文搜索的相关方法
#### 4.1 match
参数：
* analyzer
* auto_generate_synonyms_phrase_query
* fuzziness
* max_expansions
* prefix_length
* fuzzy_transpositions
* fuzzy_rewrite
* lenient
* operator
* minimum_should_match
* zero_terms_query
#### 4.2 match_phrase
* slop 
* analyzer 
* zero_terms_query
#### 4.3 match_phrase_prefix

#### 4.4 multi_match
相关查询方式：
* best_fields
* most_fields
* cross_fields
* phrase
* phrase_prefix
* bool_prefix
#### 4.5 match_bool_prefix

#### 4.6 query_string
* query string syntax
* field names
* wildcards
* regular expressions
* fuzziness
* proximity search
* ranges
* boosting
* boolean operators
* grouping
* reserved characters
* search multiple fields
* synonyms 
* minimum_should_match
#### 4.7 simple_query_string
* simple query string syntax
* limit operators
* wildcards and per-field boosts in the fields parameter
* multi-position tokens
#### 4.8 intervals
* match
* prefix
* wildcard
* fuzzy
* all_of
* any_of
#### 4.9 combined fields
与 multi_match 的比较
### 5 lucene中的精确搜索和全文匹配
* token的要素
* tokenization
* parser
#### 5.1 lucene的查询基本类
#### 5.2 lucene的queryparser

